{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Kaggle Project 2: Adult Income Classification\n",
    "Don Krapohl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "I used looping to try out many hyperparameters to choose the best model over the preprocessed data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Predict the income level of individuals based on various demographic and personal information.\n",
    "\n",
    "- Split the provided dataset into a suitable training set and testing set.\n",
    "- Train various classifiers on your training set, including Logistic Regression, SVM, Decision Trees, KNN, and Random Forest.\n",
    "- Compare the performance of the classifiers and submit your best score.\n",
    "- Aim to beat the benchmark performance of 78% on the hidden test dataset. If your performance is lower than 78%, you will lose the entire 70% of the grade allocated for the coding portion of this assignment.\n",
    "- Note that the provided test dataset (`test.csv`) does not have target variables and is solely for testing your submission within our Kaggle system. Your training dataset (`train.csv`) is all you have, so split it appropriately for training and evaluation purposes.\n",
    "- Submit your final notebook to Canvas for peer-review.\n",
    "- In your notebook, use markdown to explain your steps, rationale, and exploration of the model's performance on various classifiers. Clearly mention which classifier you have decided on and your rationale to submit it as your final submission on Kaggle.\n",
    "- Your Kaggle team name should be exactly identical to your name in Canvas.\n",
    "- Your notebook submitted on Canvas will be peer-reviewed for further evaluation.\n",
    "- Submit your submission file on Kaggle and notebook on Canvas by Nov 17, 2024, 11:59 PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be training, testing, and using the following models:\n",
    "\n",
    "* Logistic Regression\n",
    "* SVM\n",
    "* Decision Trees\n",
    "* Random Forest\n",
    "* KNN\n",
    "\n",
    "Ref: https://scikit-learn.org/1.5/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "I will:\n",
    "* load from train.csv\n",
    "* explore the data for quality and distribution\n",
    "* scale and encode the features\n",
    "* do feature selection to reduce dimensionality\n",
    "* split the data into train and test sets\n",
    "* train all the models and select the one with the best accuracy\n",
    "    \n",
    "I'll train multiple models for each of the classifier algorithms and capture the one of each type that has the highest accuracy. After all are trained and tested I'll select the one with the highest accuracy and highest AUC as my submission.  I'll then predict over the test.csv file and submit the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish environment\n",
    "1. Download the python venv for the project from https://github.com/dkrapohl/uwf-venv-breast-cancer/tree/main (I'm using the same one from project 1)\n",
    "2. Activate the environment using the README from that repo\n",
    "3. Set the Jupyter environment to use this kernel (top right of this window)\n",
    "\n",
    "If we need to reproduce the environment the private repo for this notebook has a pip requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic dataframe and operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# manipulation and preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# measuring results\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "# warning suppression\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress ConvergenceWarnings and UserWarning.  They're noise here.\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Method: measure_model\n",
    "    Purpose: output metrics about the selected model to allow model selection\n",
    "    Parameters:\n",
    "        best_model_params_info (string) - a string telling about the hyperparameters, solver, etc. as appropriate\n",
    "        model_accuracy (double) - a numeric value having the accuracy of the passed model\n",
    "        df_y_test (dataframe) - a dataframe holding the actual y values used to test\n",
    "        dt_test_predictions (dataframe) - a dataframe containing the predicted y values to calculate scores\n",
    "'''\n",
    "def measure_model(best_model_params_info, model_accuracy, df_y_test, df_test_predictions):\n",
    "    # Print the best model info\n",
    "    print(\"Best model: {} , accuracy {}\".format(best_model_params_info, \"{:.4f}\".format(model_accuracy)))\n",
    "    print(\"MAE: {}, AUC: {}\".format(mean_absolute_error(df_y_test, df_test_predictions), \n",
    "                                       roc_auc_score(df_y_test,df_test_predictions)))\n",
    "    print(\"Precision: {}, Recall: {}\".format(precision_score(df_y_test, df_test_predictions), \n",
    "                                       recall_score(df_y_test,df_test_predictions)))    \n",
    "    # Print the confusion matrix for the best model\n",
    "    print(\"Model Confusion Matrix:\\n\", confusion_matrix(df_y_test, df_test_predictions))\n",
    "    print(\"Train data F1-Score for class '1':\", f1_score(df_y_test, df_test_predictions, pos_label=1))\n",
    "    print(\"Train data F1-Score for class '0':\", f1_score(df_y_test, df_test_predictions, pos_label=0))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a few collections to capture the info on our different models\n",
    "\n",
    "To explore the accuracies of multiple model hyperparameters I'll be training multiple models and keeping the model from each type of classifier that has the highest accuracy.  I suspect there's an easier way to accomplish this but this is what I can do at this point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collections we'll use for our best of each type of model\n",
    "best_models = []                    # List of model instances that are our best for final evaluation\n",
    "model_accuracies = []               # The accuracies of our best models in a key-value dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll load the data into an initial dataframe to be used for exploration and the start of preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39073, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the csv training dataset to a pandas dataframe\n",
    "# The dataset is expected in the same directory as this notebook\n",
    "#   under a subfolder path datasets/\n",
    "data_train = pd.read_csv('datasets/train.csv')\n",
    "# Show the shape of the dataset\n",
    "data_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Output the first 5 rows of the data to see the general character and nature of the data like missing values, obvious dirty data, features with very large ranges, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78</td>\n",
       "      <td>Private</td>\n",
       "      <td>111189</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>Dominican-Republic</td>\n",
       "      <td>0</td>\n",
       "      <td>26052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>122066</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>47049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>168682</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>33915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>Private</td>\n",
       "      <td>110230</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>22132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>373050</td>\n",
       "      <td>12th</td>\n",
       "      <td>8</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>46452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt     education  educational-num  \\\n",
       "0   78           Private  111189       7th-8th                4   \n",
       "1   49      Self-emp-inc  122066  Some-college               10   \n",
       "2   62  Self-emp-not-inc  168682       7th-8th                4   \n",
       "3   18           Private  110230          10th                6   \n",
       "4   40           Private  373050          12th                8   \n",
       "\n",
       "       marital-status         occupation   relationship   race  gender  \\\n",
       "0       Never-married  Machine-op-inspct  Not-in-family  White  Female   \n",
       "1            Divorced              Sales  Not-in-family  White    Male   \n",
       "2  Married-civ-spouse              Sales        Husband  White    Male   \n",
       "3       Never-married      Other-service      Own-child  White    Male   \n",
       "4  Married-civ-spouse      Other-service        Husband  White    Male   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week      native-country  income  \\\n",
       "0             0             0              35  Dominican-Republic       0   \n",
       "1             0             0              25       United-States       0   \n",
       "2             0             0               5       United-States       0   \n",
       "3             0             0              11       United-States       0   \n",
       "4             0             0              40                   ?       0   \n",
       "\n",
       "      id  \n",
       "0  26052  \n",
       "1  47049  \n",
       "2  33915  \n",
       "3  22132  \n",
       "4  46452  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a few rows from the training data\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that there are a lot of columns to encode. Also, education appears to be ordinal but there's a numeric version of it as well, which should cancel out as having low information value when I apply PCA and/or LDA. The id columns needs to be removed as well.\n",
    "\n",
    "I also note that the \"income\" column is already discretized into two buckets with the bucket names already encoded as 0 and 1.  Per the documentation, column info:\n",
    "\n",
    "    income: Target variable column indicating the income level ('<=50K': 0 or '>50K': 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data to make sure we don't have null or missing data\n",
    "\n",
    "This will give the count of null values for each column to see if we need to handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                0\n",
       "workclass          0\n",
       "fnlwgt             0\n",
       "education          0\n",
       "educational-num    0\n",
       "marital-status     0\n",
       "occupation         0\n",
       "relationship       0\n",
       "race               0\n",
       "gender             0\n",
       "capital-gain       0\n",
       "capital-loss       0\n",
       "hours-per-week     0\n",
       "native-country     0\n",
       "income             0\n",
       "id                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the count of nulls per column\n",
    "# Turns out we don't have any\n",
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                0\n",
       "workclass          0\n",
       "fnlwgt             0\n",
       "education          0\n",
       "educational-num    0\n",
       "marital-status     0\n",
       "occupation         0\n",
       "relationship       0\n",
       "race               0\n",
       "gender             0\n",
       "capital-gain       0\n",
       "capital-loss       0\n",
       "hours-per-week     0\n",
       "native-country     0\n",
       "income             0\n",
       "id                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the count of nulls per column\n",
    "# Also, no nulls here\n",
    "data_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I prefer to verify object data types. Not important here but at scale it definitely is.\n",
    "\n",
    "Sometimes numeric data come in as object, which can make lookups and indexing inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39073 entries, 0 to 39072\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   age              39073 non-null  int64 \n",
      " 1   workclass        39073 non-null  object\n",
      " 2   fnlwgt           39073 non-null  int64 \n",
      " 3   education        39073 non-null  object\n",
      " 4   educational-num  39073 non-null  int64 \n",
      " 5   marital-status   39073 non-null  object\n",
      " 6   occupation       39073 non-null  object\n",
      " 7   relationship     39073 non-null  object\n",
      " 8   race             39073 non-null  object\n",
      " 9   gender           39073 non-null  object\n",
      " 10  capital-gain     39073 non-null  int64 \n",
      " 11  capital-loss     39073 non-null  int64 \n",
      " 12  hours-per-week   39073 non-null  int64 \n",
      " 13  native-country   39073 non-null  object\n",
      " 14  income           39073 non-null  int64 \n",
      " 15  id               39073 non-null  int64 \n",
      "dtypes: int64(8), object(8)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Verify data types to see if there's a better explicit cast for any feature or if we need to encode anything\n",
    "data_train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Need to encode (index):\n",
    "> workclass - 1  \n",
    "> education - 3  \n",
    "> marital-status - 4  \n",
    "> occupation - 6  \n",
    "> relationship - 7  \n",
    "> race - 8  \n",
    "> gender - 9  \n",
    "> native-country - 13  \n",
    "    \n",
    "The other features need to be scaled (index 0, 2, 4, 10, 11, 12, 14, 15)  \n",
    "\n",
    "Income probably needs to be discretized (14)  \n",
    "\n",
    "ID needs to be removed from training (15)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the statistics about the data and their distribution.\n",
    "\n",
    "I'm looking here for any columns with differing counts and any outrageous outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39073.000000</td>\n",
       "      <td>3.907300e+04</td>\n",
       "      <td>39073.000000</td>\n",
       "      <td>39073.000000</td>\n",
       "      <td>39073.000000</td>\n",
       "      <td>39073.000000</td>\n",
       "      <td>39073.000000</td>\n",
       "      <td>39073.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.588207</td>\n",
       "      <td>1.900714e+05</td>\n",
       "      <td>10.072556</td>\n",
       "      <td>1067.195327</td>\n",
       "      <td>86.108796</td>\n",
       "      <td>40.390269</td>\n",
       "      <td>0.238144</td>\n",
       "      <td>24465.705372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.695509</td>\n",
       "      <td>1.059839e+05</td>\n",
       "      <td>2.570352</td>\n",
       "      <td>7426.475044</td>\n",
       "      <td>399.342390</td>\n",
       "      <td>12.335446</td>\n",
       "      <td>0.425953</td>\n",
       "      <td>14072.213508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.349200e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.175560e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.784780e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24492.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.383670e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36610.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.490400e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  educational-num  capital-gain  \\\n",
       "count  39073.000000  3.907300e+04     39073.000000  39073.000000   \n",
       "mean      38.588207  1.900714e+05        10.072556   1067.195327   \n",
       "std       13.695509  1.059839e+05         2.570352   7426.475044   \n",
       "min       17.000000  1.349200e+04         1.000000      0.000000   \n",
       "25%       28.000000  1.175560e+05         9.000000      0.000000   \n",
       "50%       37.000000  1.784780e+05        10.000000      0.000000   \n",
       "75%       48.000000  2.383670e+05        12.000000      0.000000   \n",
       "max       90.000000  1.490400e+06        16.000000  99999.000000   \n",
       "\n",
       "       capital-loss  hours-per-week        income            id  \n",
       "count  39073.000000    39073.000000  39073.000000  39073.000000  \n",
       "mean      86.108796       40.390269      0.238144  24465.705372  \n",
       "std      399.342390       12.335446      0.425953  14072.213508  \n",
       "min        0.000000        1.000000      0.000000      1.000000  \n",
       "25%        0.000000       40.000000      0.000000  12319.000000  \n",
       "50%        0.000000       40.000000      0.000000  24492.000000  \n",
       "75%        0.000000       45.000000      0.000000  36610.000000  \n",
       "max     4356.000000       99.000000      1.000000  48842.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display basic metrics about each feature, like count, mean, std, min/max, and IQR\n",
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that some of the features, such as capital gain/loss have huge variability with the IQR being 0 but the max is 4356."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust columns as needed\n",
    "\n",
    "1. Remove ID and make a dataframe of our target predictions. We need to remove income and ID from the training data. The former is the has no information and the latter is the value we're predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the result column from the input parameters\n",
    "# also remove the ID column. It carries no signal.\n",
    "X_train_without_label = data_train.drop('income', axis=1).drop('id', axis=1)\n",
    "\n",
    "# Assign the variable we're targeting for the input data\n",
    "y_values = data_train['income']      # assign the income continuous variable we'll discretize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encode categorical values - I'll use the ColumnTransformer to encode multiple columns here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and scale the data\n",
    "\n",
    "Here I'll be splitting the data to be 70% training set, 30% test set. I then scale the data using the Standard Scaler to make all the data within the same range having 0 as the mean and standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do train test split\n",
    "X_train, X_test, y_train, y_test =    train_test_split(X_train_without_label, y_values,\n",
    "    test_size=0.3, \n",
    "    random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a list of string categorical cols we need to encode\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
    "# Make a list of features we need to scale\n",
    "numeric_features = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "ohe = OneHotEncoder()           # creating a one hot encoder to do our column transforms\n",
    "ss = StandardScaler()           # creating a scaler for transforming\n",
    "\n",
    "feature_encoder = ColumnTransformer(\n",
    "    transformers = [('onehot', ohe, categorical_features), \n",
    "                    ('scaler', ss, numeric_features)],\n",
    "    remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "X_train_std = feature_encoder.fit_transform(X_train)\n",
    "X_test_std = feature_encoder.transform(X_test)\n",
    "\n",
    "# Here I'm not specifying the number of components.\n",
    "# I want to chart and see how many to choose\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.fit(X_test_std)\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "X_train_pca.shape[1]\n",
    "#df_transformed = pd.DataFrame(\n",
    "#    X_transformed,\n",
    "#    columns=feature_encoder.get_feature_names_out()\n",
    "#)\n",
    "#X_test_clean = feature_encoder.fit(X_test)\n",
    "\n",
    "#df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(1, 108), pca.explained_variance_ratio_, align='center')\n",
    "plt.step(range(1, 108), np.cumsum(pca.explained_variance_ratio_), where='mid')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we can do a reasonable job of training with just 12 components so let's set that.  I'll compare this at the end of the notebook with our selected model on the original train/test set to see how much PCA improved it to know if I went wrong with my choice of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will select 12 principal components\n",
    "pca = PCA(n_components=12)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Train and Test\n",
    "\n",
    "I'm looping through hyperparameters and training multiple models, keeping the \"best\" based on accuracy. The secondary consideration is that I will prefer the lowest C value as lower C values make simpler models.\n",
    "\n",
    "This model assumes linearity in independent variables and performs well if the data are linearly separable. It'll also give use proababilities on the class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (solver=lbfgs\tscore=0.8386\tauc=0.7406\tf1=0.6213\n",
      "Logistic Regression (solver=lbfgs\tscore=0.8386\tauc=0.7406\tf1=0.6213\n",
      "Logistic Regression (solver=lbfgs\tscore=0.8386\tauc=0.7406\tf1=0.6213\n",
      "Logistic Regression (solver=lbfgs\tscore=0.8386\tauc=0.7406\tf1=0.6213\n",
      "Logistic Regression (solver=lbfgs\tscore=0.8384\tauc=0.7403\tf1=0.6207\n",
      "Logistic Regression (solver=lbfgs\tscore=0.8384\tauc=0.7400\tf1=0.6204\n",
      "Logistic Regression (solver=lbfgs\tscore=0.8383\tauc=0.7394\tf1=0.6196\n",
      "Logistic Regression (solver=lbfgs\tscore=0.8354\tauc=0.7316\tf1=0.6077\n",
      "Logistic Regression (solver=lbfgs\tscore=0.8279\tauc=0.7019\tf1=0.5616\n",
      "Logistic Regression (solver=lbfgs\tscore=0.8009\tauc=0.5977\tf1=0.3331\n",
      "Logistic Regression (solver=liblinear\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=liblinear\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=liblinear\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=liblinear\tscore=0.8386\tauc=0.7406\tf1=0.6213\n",
      "Logistic Regression (solver=liblinear\tscore=0.8386\tauc=0.7406\tf1=0.6213\n",
      "Logistic Regression (solver=liblinear\tscore=0.8385\tauc=0.7402\tf1=0.6207\n",
      "Logistic Regression (solver=liblinear\tscore=0.8383\tauc=0.7396\tf1=0.6197\n",
      "Logistic Regression (solver=liblinear\tscore=0.8365\tauc=0.7354\tf1=0.6134\n",
      "Logistic Regression (solver=liblinear\tscore=0.8309\tauc=0.7257\tf1=0.5975\n",
      "Logistic Regression (solver=liblinear\tscore=0.8069\tauc=0.7287\tf1=0.5894\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8384\tauc=0.7404\tf1=0.6209\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8384\tauc=0.7404\tf1=0.6209\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8386\tauc=0.7404\tf1=0.6210\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8385\tauc=0.7402\tf1=0.6207\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8383\tauc=0.7396\tf1=0.6197\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8354\tauc=0.7314\tf1=0.6076\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8279\tauc=0.7019\tf1=0.5616\n",
      "Logistic Regression (solver=newton-cg\tscore=0.8010\tauc=0.5979\tf1=0.3336\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8386\tauc=0.7404\tf1=0.6210\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8386\tauc=0.7404\tf1=0.6210\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8385\tauc=0.7402\tf1=0.6207\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8383\tauc=0.7396\tf1=0.6199\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8354\tauc=0.7316\tf1=0.6078\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8279\tauc=0.7019\tf1=0.5616\n",
      "Logistic Regression (solver=newton-cholesky\tscore=0.8009\tauc=0.5977\tf1=0.3331\n",
      "Logistic Regression (solver=sag\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=sag\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=sag\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=sag\tscore=0.8386\tauc=0.7406\tf1=0.6213\n",
      "Logistic Regression (solver=sag\tscore=0.8385\tauc=0.7403\tf1=0.6209\n",
      "Logistic Regression (solver=sag\tscore=0.8385\tauc=0.7402\tf1=0.6207\n",
      "Logistic Regression (solver=sag\tscore=0.8383\tauc=0.7396\tf1=0.6197\n",
      "Logistic Regression (solver=sag\tscore=0.8354\tauc=0.7316\tf1=0.6078\n",
      "Logistic Regression (solver=sag\tscore=0.8279\tauc=0.7019\tf1=0.5616\n",
      "Logistic Regression (solver=sag\tscore=0.8009\tauc=0.5977\tf1=0.3331\n",
      "Logistic Regression (solver=saga\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=saga\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=saga\tscore=0.8385\tauc=0.7406\tf1=0.6212\n",
      "Logistic Regression (solver=saga\tscore=0.8386\tauc=0.7406\tf1=0.6213\n",
      "Logistic Regression (solver=saga\tscore=0.8385\tauc=0.7403\tf1=0.6209\n",
      "Logistic Regression (solver=saga\tscore=0.8385\tauc=0.7402\tf1=0.6207\n",
      "Logistic Regression (solver=saga\tscore=0.8383\tauc=0.7396\tf1=0.6199\n",
      "Logistic Regression (solver=saga\tscore=0.8354\tauc=0.7316\tf1=0.6078\n",
      "Logistic Regression (solver=saga\tscore=0.8279\tauc=0.7019\tf1=0.5616\n",
      "Logistic Regression (solver=saga\tscore=0.8009\tauc=0.5977\tf1=0.3331\n"
     ]
    }
   ],
   "source": [
    "# In a loop over several values, train an SVM with different C values and capture the accuracies\n",
    "# We prefer lower C for better power so we'll only update if accuracy is higher and C lower\n",
    "\n",
    "solvers = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "cs = [5.0, 3.0, 2.0, 1.0, 0.75, 0.5, 0.1, 0.01, 0.001, 0.0001]  # C values to try\n",
    "\n",
    "# I'll also keep track of the highest accuracy for the lowest C as our \"best\" model\n",
    "lr_highest_accuracy=0\n",
    "lr_lowest_c=1000.0\n",
    "\n",
    "# Try all SVM kernels\n",
    "for solver in solvers:\n",
    "    # Try the range of C values\n",
    "    for c in cs:\n",
    "        # Initialize and train the SVM model\n",
    "        logreg_model = LogisticRegression(solver=solver, C=c, random_state = 17)\n",
    "        logreg_model.fit(X_train_pca, y_train)\n",
    "        \n",
    "        # Collect info on training results if desired\n",
    "        # model_train_score = logreg_model.score(X_train_scaled, y_train)                    # get the model accuracy\n",
    "        # print(\"{}\\t{}\".format(\"Logistic Regression (solver={})\".format(solver), \"{:.4f}\".format(model_test_score)))\n",
    "            \n",
    "        # Make predictions using training data\n",
    "        y_pred = logreg_model.predict(X_test_pca)\n",
    "        model_test_score = accuracy_score(y_test, y_pred)\n",
    "        model_auc = roc_auc_score(y_test, y_pred)\n",
    "        model_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Print the accuracies for each of the model params so far.   \n",
    "        print(\"Logistic Regression (solver={}\\tscore={}\\tauc={}\\tf1={}\".format(solver, \"{:.4f}\".format(model_test_score),\n",
    "                                  \"{:.4f}\".format(model_auc), \"{:.4f}\".format(model_f1)))\n",
    "            \n",
    "        # we want the lowest C for better generalization so only keep\n",
    "        #   accuracy if it's better but C is lower\n",
    "        if model_test_score >= lr_highest_accuracy: # we're in a list with decreasing values so don't need to check C\n",
    "            test_predictions = y_pred                   # store test predictions\n",
    "            lr_lowest_c = c                             # store the lowest C\n",
    "            lr_best_model = logreg_model                # store the best model\n",
    "            lr_best_solver = solver                     # store the best solver\n",
    "            lr_highest_accuracy = model_test_score       # update our highest score\n",
    "\n",
    "best_models.append(lr_best_model)                       # add to our \"best model\" collection      \n",
    "model_accuracies.append(lr_highest_accuracy)                # add the accuracy      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: saga solver, c 0.0001 , accuracy 0.8386\n",
      "MAE: 0.1614059034294489, AUC: 0.7406327699944444\n",
      "Precision: 0.7096479195244627, Recall: 0.5525097899608401\n",
      "Model Confusion Matrix:\n",
      " [[8278  635]\n",
      " [1257 1552]]\n",
      "Train data F1-Score for class '1': 0.6212970376301041\n",
      "Train data F1-Score for class '0': 0.897441457068517\n"
     ]
    }
   ],
   "source": [
    "# Print the best model info\n",
    "model_param_info =\"{} solver, c {}\".format(lr_best_solver, c, \"{:.4f}\")\n",
    "\n",
    "measure_model(model_param_info, lr_highest_accuracy, y_test, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Train and Test\n",
    "\n",
    "As with Logistic Regression, looping to train multiple models, keeping the one with the highest accuracy for the lowest C value.  C value again we prefer lower C as it makes for simpler models.\n",
    "\n",
    "This model will be computationally more expensive than the others, especially since I'm testing multiple C values on all kernels. The algorithm is more flexible in its ability to model both linear and non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM kernel=linear, C=5.0, score=0.8416, aud=0.7438, f1=0.6272\n",
      "SVM kernel=linear, C=3.0, score=0.8416, aud=0.7438, f1=0.6272\n"
     ]
    }
   ],
   "source": [
    "# In a loop over several values, train an SVM with different C values and capture the accuracies\n",
    "# We prefer lower C for better power so we'll only update if accuracy is higher and C lower\n",
    "cs = [5.0, 3.0, 2.0, 1.0, 0.75, 0.5, 0.1, 0.01, 0.001, 0.0001]  # C values to try\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "\n",
    "# I'll also keep track of the highest accuracy for the highest eta as our \"best\" model\n",
    "svm_highest_accuracy=0\n",
    "svm_lowest_c=1000\n",
    "\n",
    "# Try all SVM kernels\n",
    "for kernel in kernels:\n",
    "    # Loop through C values largest to smallest, train and test each\n",
    "    for c in cs:\n",
    "        # Initialize and train the SVM model\n",
    "        linear_svm = SVC(kernel=kernel, C=c, random_state=17)\n",
    "        linear_svm.fit(X_train_pca, y_train)\n",
    "        \n",
    "        # Collect info on training results if desired\n",
    "        # model_train_score = linear_svm.score(X_train_scaled, y_train)                    # get the model accuracy\n",
    "        # print(\"{}\\t{}\".format(\"SVM Train (kernel={}, C={})\".format(kernel, c), \"{:.4f}\".format(model_test_score)))\n",
    "            \n",
    "        # Make predictions using training data\n",
    "        y_pred = linear_svm.predict(X_test_pca)\n",
    "        model_test_score = accuracy_score(y_test, y_pred)\n",
    "        model_auc = roc_auc_score(y_test, y_pred)\n",
    "        model_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Print the accuracies for each of the model params so far.   \n",
    "        print(\"SVM kernel={}, C={}, score={}, auc={}, f1={}\".format(kernel, c, \"{:.4f}\".format(model_test_score),\n",
    "                                  \"{:.4f}\".format(model_auc), \"{:.4f}\".format(model_f1)))\n",
    "            \n",
    "        # we want the lowest C for better generalization so only keep\n",
    "        #   accuracy if it's better but C is lower\n",
    "        if model_test_score >= svm_highest_accuracy: # we're in a list with decreasing values so don't need to check C\n",
    "            svm_lowest_c = c                            # store lowest C\n",
    "            test_predictions = y_pred                   # store test predictions\n",
    "            svm_best_model = linear_svm                 # store the best model\n",
    "            svm_best_kernel = kernel                    # store the best kernel\n",
    "            svm_highest_accuracy = model_test_score     # update our highest score\n",
    "\n",
    "best_models.append(svm_best_model)                      # add to our \"best model\" collection   \n",
    "model_accuracies.append(svm_highest_accuracy)           # add the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: rbf kernel, C 5.0 , accuracy 0.8497\n",
      "MAE: 0.15031564579423307, AUC: 0.7473159812286141\n",
      "Precision: 0.7557401074743527, Recall: 0.5507297970808117\n",
      "Model Confusion Matrix:\n",
      " [[8413  500]\n",
      " [1262 1547]]\n",
      "Train data F1-Score for class '1': 0.6371499176276771\n",
      "Train data F1-Score for class '0': 0.9052076608564665\n"
     ]
    }
   ],
   "source": [
    "# Print the best model info\n",
    "model_param_info =\"{} kernel, C {}\".format(svm_best_kernel, svm_lowest_c, \"{:.4f}\")\n",
    "measure_model(model_param_info, svm_highest_accuracy, y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees Train and Test\n",
    "\n",
    "With Decision Trees we're testing different splitting algorithms as well as model depth keeping the one with the highest accuracy and the lowest depth.  Lowest depth is selected as it makes for the simpler model.\n",
    "\n",
    "This algorithm should be the easiest to interpret and captures non-linear relationships but it's prone to overfitting.  It's also at risk of creating biased trees if the classes are imbalanced but the classes here are relatively equally represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (criterion=gini, depth=50)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=49)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=48)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=47)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=46)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=45)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=44)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=43)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=42)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=41)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=40)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=39)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=38)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=37)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=36)\t0.7917\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=35)\t0.7906\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=34)\t0.7886\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=33)\t0.7903\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=32)\t0.7893\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=31)\t0.7907\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=30)\t0.7913\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=29)\t0.7912\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=28)\t0.7897\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=27)\t0.7896\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=26)\t0.7883\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=25)\t0.7916\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=24)\t0.7953\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=23)\t0.7912\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=22)\t0.7895\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=21)\t0.7936\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=20)\t0.7957\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=19)\t0.7955\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=18)\t0.7954\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=17)\t0.7983\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=16)\t0.8000\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=15)\t0.8037\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=14)\t0.8121\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=13)\t0.8135\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=12)\t0.8199\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=11)\t0.8261\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=10)\t0.8258\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=9)\t0.8316\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=8)\t0.8341\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=7)\t0.8351\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=6)\t0.8318\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=5)\t0.8277\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=4)\t0.8207\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=3)\t0.8174\t0.5977\t0.3331\n",
      "Decision Tree (criterion=gini, depth=2)\t0.8142\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=50)\t0.7947\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=49)\t0.7947\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=48)\t0.7947\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=47)\t0.7944\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=46)\t0.7961\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=45)\t0.7952\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=44)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=43)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=42)\t0.7960\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=41)\t0.7944\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=40)\t0.7945\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=39)\t0.7943\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=38)\t0.7960\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=37)\t0.7965\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=36)\t0.7954\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=35)\t0.7961\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=34)\t0.7980\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=33)\t0.7965\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=32)\t0.7954\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=31)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=30)\t0.7954\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=29)\t0.7951\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=28)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=27)\t0.7975\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=26)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=25)\t0.8007\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=24)\t0.7961\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=23)\t0.7988\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=22)\t0.7994\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=21)\t0.8023\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=20)\t0.8013\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=19)\t0.8012\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=18)\t0.8052\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=17)\t0.8065\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=16)\t0.8086\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=15)\t0.8096\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=14)\t0.8151\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=13)\t0.8168\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=12)\t0.8197\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=11)\t0.8226\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=10)\t0.8286\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=9)\t0.8330\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=8)\t0.8336\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=7)\t0.8369\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=6)\t0.8313\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=5)\t0.8245\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=4)\t0.8133\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=3)\t0.8163\t0.5977\t0.3331\n",
      "Decision Tree (criterion=entropy, depth=2)\t0.7997\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=50)\t0.7947\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=49)\t0.7947\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=48)\t0.7947\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=47)\t0.7944\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=46)\t0.7961\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=45)\t0.7952\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=44)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=43)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=42)\t0.7960\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=41)\t0.7944\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=40)\t0.7945\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=39)\t0.7943\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=38)\t0.7960\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=37)\t0.7965\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=36)\t0.7954\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=35)\t0.7961\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=34)\t0.7980\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=33)\t0.7965\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=32)\t0.7954\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=31)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=30)\t0.7954\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=29)\t0.7951\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=28)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=27)\t0.7975\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=26)\t0.7959\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=25)\t0.8007\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=24)\t0.7961\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=23)\t0.7988\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=22)\t0.7994\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=21)\t0.8023\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=20)\t0.8013\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=19)\t0.8012\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=18)\t0.8052\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=17)\t0.8065\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=16)\t0.8086\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=15)\t0.8096\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=14)\t0.8151\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=13)\t0.8168\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=12)\t0.8197\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=11)\t0.8226\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=10)\t0.8286\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=9)\t0.8330\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=8)\t0.8336\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=7)\t0.8369\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=6)\t0.8313\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=5)\t0.8245\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=4)\t0.8133\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=3)\t0.8163\t0.5977\t0.3331\n",
      "Decision Tree (criterion=log_loss, depth=2)\t0.7997\t0.5977\t0.3331\n"
     ]
    }
   ],
   "source": [
    "# In a loop over several values, train a decision tree with different split criteria and depths and capture the accuracies\n",
    "# We prefer lower C for better power so we'll only update if accuracy is higher and depth lower\n",
    "criteria = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "# I'll also keep track of the highest accuracy for the lowest depth as our \"best\" model\n",
    "dtree_highest_accuracy=0\n",
    "dtree_lowest_depth=1000\n",
    "dtree_best_criterion=''\n",
    "\n",
    "# Try all SVM kernels\n",
    "for criterion in criteria:\n",
    "    # Loop through depth values smallest to largest, train and test each\n",
    "    for depth in range(50, 1, -1):   # Allow up to depth 5 starting at 5 and down to 1\n",
    "        # Initialize and train the decision tree model\n",
    "        decision_tree = DecisionTreeClassifier(criterion=criterion, max_depth=depth, random_state=17)\n",
    "        decision_tree.fit(X_train_pca, y_train)\n",
    "        \n",
    "        # Collect info on training results if desired\n",
    "        # model_train_score = decision_tree.score(X_train_scaled, y_train)                    # get the model accuracy\n",
    "        # print(\"{}\\t{}\".format(\"Decision Tree Train (criterion={}, depth={})\".format(criterion, depth), \"{:.4f}\".format(model_test_score)))\n",
    "            \n",
    "        # Make predictions using training data\n",
    "        y_pred = decision_tree.predict(X_test_pca)\n",
    "        model_test_score = accuracy_score(y_test, y_pred)\n",
    "        model_auc = roc_auc_score(y_test, y_pred)\n",
    "        model_f1 = f1_score(y_test, y_pred)        \n",
    "\n",
    "        # Print the accuracies for each of the model params so far.   \n",
    "        print(\"Decision Tree (criterion={}, depth={}, score={}, auc={}, f1={})\".format(criterion, depth, \"{:.4f}\".format(model_test_score),\n",
    "                                  \"{:.4f}\".format(model_auc), \"{:.4f}\".format(model_f1)))\n",
    "            \n",
    "        # we want the lowest C for better generalization so only keep\n",
    "        #   accuracy if it's better but C is lower\n",
    "        if model_test_score >= dtree_highest_accuracy:\n",
    "            dtree_lowest_depth = depth              # store lowest depth\n",
    "            test_predictions = y_pred               # store test predictions\n",
    "            dtree_best_model = decision_tree        # store the best model\n",
    "            dtree_best_criterion = criterion        # store the best kernel\n",
    "            dtree_highest_accuracy = model_test_score   # update our highest score\n",
    "\n",
    "best_models.append(dtree_best_model)                # add to our \"best model\" collection\n",
    "model_accuracies.append(dtree_highest_accuracy)     # add the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: log_loss criterion, depth 7 , accuracy 0.8369\n",
      "MAE: 0.1631120969117898, AUC: 0.7325624304593548\n",
      "Precision: 0.7142857142857143, Recall: 0.5322178711285155\n",
      "Model Confusion Matrix:\n",
      " [[8315  598]\n",
      " [1314 1495]]\n",
      "Train data F1-Score for class '1': 0.6099551203590371\n",
      "Train data F1-Score for class '0': 0.8968827526696149\n"
     ]
    }
   ],
   "source": [
    "# Print the best model info\n",
    "model_param_info =\"{} criterion, depth {}\".format(dtree_best_criterion, dtree_lowest_depth)\n",
    "measure_model(model_param_info, dtree_highest_accuracy, y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Train and Test\n",
    "\n",
    "This is very much like Decision Trees except we're tuning splitting criteria as well as the number of estimators used.  We'll prefer fewer estimators for simpler models.\n",
    "\n",
    "The Random Forest algorithm is harder to interpret because it's an ensemble of decision trees but it's less likely to overfit. It can be harder to compute than a single or a few decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test (criterion=gini, estimators=500)\t0.8446\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=gini, estimators=450)\t0.8449\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=gini, estimators=400)\t0.8452\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=gini, estimators=350)\t0.8451\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=gini, estimators=300)\t0.8453\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=gini, estimators=250)\t0.8451\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=gini, estimators=200)\t0.8441\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=gini, estimators=150)\t0.8435\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=gini, estimators=100)\t0.8423\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=entropy, estimators=500)\t0.8441\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=entropy, estimators=450)\t0.8436\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=entropy, estimators=400)\t0.8428\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=entropy, estimators=350)\t0.8433\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=entropy, estimators=300)\t0.8441\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=entropy, estimators=250)\t0.8446\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=entropy, estimators=200)\t0.8452\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=entropy, estimators=150)\t0.8439\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=entropy, estimators=100)\t0.8426\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=log_loss, estimators=500)\t0.8441\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=log_loss, estimators=450)\t0.8436\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=log_loss, estimators=400)\t0.8428\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=log_loss, estimators=350)\t0.8433\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=log_loss, estimators=300)\t0.8441\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=log_loss, estimators=250)\t0.8446\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=log_loss, estimators=200)\t0.8452\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=log_loss, estimators=150)\t0.8439\t0.5977\t0.3331\n",
      "Random Forest Test (criterion=log_loss, estimators=100)\t0.8426\t0.5977\t0.3331\n"
     ]
    }
   ],
   "source": [
    "# In a loop over several values, train a random forest with different split criteria and estimator count and capture the accuracies\n",
    "criteria = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "# I'll also keep track of the highest accuracy for the lowest number of estimators as our \"best\" model\n",
    "rforest_highest_accuracy=0\n",
    "rforest_lowest_estimators=1000\n",
    "rforest_best_criterion=''\n",
    "\n",
    "# Try all random forest split criteria\n",
    "for criterion in criteria:\n",
    "    # Loop through depth values smallest to largest, train and test each\n",
    "    for estimators in range(500, 50, -50):   # Allow up to 500 estimators decreasing by 50 each loop\n",
    "        # Initialize and train the decision tree model\n",
    "        random_forest = RandomForestClassifier(criterion=criterion, n_estimators=estimators, random_state=17)\n",
    "        random_forest.fit(X_train_pca, y_train)\n",
    "        \n",
    "        # Collect info on training results if desired\n",
    "        # model_train_score = random_forest.score(X_train_scaled, y_train)                    # get the model accuracy\n",
    "        # print(\"{}\\t{}\".format(\"Random Forest Train (criterion={}, estimators={})\".format(criterion, estimators), \"{:.4f}\".format(model_test_score)))\n",
    "            \n",
    "        # Make predictions using training data\n",
    "        y_pred = random_forest.predict(X_test_pca)\n",
    "        model_test_score = accuracy_score(y_test, y_pred)\n",
    "        model_auc = roc_auc_score(y_test, y_pred)\n",
    "        model_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Print the accuracies for each of the model params so far.   \n",
    "        print(\"Random Forest Test (criterion={}, estimators={}, score={}, auc={}, f1={})\".format(criterion, estimators, \n",
    "              \"{:.4f}\".format(model_test_score), \"{:.4f}\".format(model_auc), \"{:.4f}\".format(model_f1)))\n",
    "            \n",
    "        # we want the lowest number of estimators for better generalization \n",
    "        if model_test_score >= rforest_highest_accuracy:\n",
    "            rforest_lowest_estimators = estimators  # store lowest number of estimators\n",
    "            test_predictions = y_pred               # store test predictions\n",
    "            rforest_best_model = random_forest      # store the best model\n",
    "            rforest_best_criterion = criterion      # store the best criterion\n",
    "            rforest_highest_accuracy = model_test_score # update our highest score\n",
    "            \n",
    "best_models.append(rforest_best_model)              # add to our \"best model\" collection        \n",
    "model_accuracies.append(rforest_highest_accuracy)   # add the accuracy    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: gini criterion, estimators 300 , accuracy 0.8453\n",
      "MAE: 0.15466643917420236, AUC: 0.7588393631615644\n",
      "Precision: 0.7133676092544987, Recall: 0.5927376290494838\n",
      "Model Confusion Matrix:\n",
      " [[8244  669]\n",
      " [1144 1665]]\n",
      "Train data F1-Score for class '1': 0.6474820143884892\n",
      "Train data F1-Score for class '0': 0.9009343751707557\n"
     ]
    }
   ],
   "source": [
    "# Print the best model info\n",
    "model_param_info =\"{} criterion, estimators {}\".format(rforest_best_criterion, rforest_lowest_estimators)\n",
    "measure_model(model_param_info, rforest_highest_accuracy, y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Train and Test\n",
    "\n",
    "This is simpler as I'm only tuning the number of neighbors and keeping the one with the highest accuracy. We'll prefer the model with the fewest neighbors.\n",
    "\n",
    "KNN, while computational a bit expensive is simple for this problem and doesn't appear to require much tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test (neighbors=2)\t0.8179\t0.5977\t0.3331\n",
      "KNN Test (neighbors=3)\t0.8234\t0.5977\t0.3331\n",
      "KNN Test (neighbors=4)\t0.8280\t0.5977\t0.3331\n",
      "KNN Test (neighbors=5)\t0.8314\t0.5977\t0.3331\n",
      "KNN Test (neighbors=6)\t0.8296\t0.5977\t0.3331\n",
      "KNN Test (neighbors=7)\t0.8343\t0.5977\t0.3331\n",
      "KNN Test (neighbors=8)\t0.8357\t0.5977\t0.3331\n",
      "KNN Test (neighbors=9)\t0.8359\t0.5977\t0.3331\n",
      "KNN Test (neighbors=10)\t0.8375\t0.5977\t0.3331\n",
      "KNN Test (neighbors=11)\t0.8385\t0.5977\t0.3331\n",
      "KNN Test (neighbors=12)\t0.8396\t0.5977\t0.3331\n",
      "KNN Test (neighbors=13)\t0.8405\t0.5977\t0.3331\n",
      "KNN Test (neighbors=14)\t0.8412\t0.5977\t0.3331\n",
      "KNN Test (neighbors=15)\t0.8406\t0.5977\t0.3331\n",
      "KNN Test (neighbors=16)\t0.8396\t0.5977\t0.3331\n",
      "KNN Test (neighbors=17)\t0.8399\t0.5977\t0.3331\n",
      "KNN Test (neighbors=18)\t0.8397\t0.5977\t0.3331\n",
      "KNN Test (neighbors=19)\t0.8398\t0.5977\t0.3331\n",
      "KNN Test (neighbors=20)\t0.8395\t0.5977\t0.3331\n",
      "KNN Test (neighbors=21)\t0.8391\t0.5977\t0.3331\n",
      "KNN Test (neighbors=22)\t0.8400\t0.5977\t0.3331\n",
      "KNN Test (neighbors=23)\t0.8379\t0.5977\t0.3331\n",
      "KNN Test (neighbors=24)\t0.8399\t0.5977\t0.3331\n"
     ]
    }
   ],
   "source": [
    "# In a loop over several values, train a knn movel with different number of neighbors\n",
    "\n",
    "# I'll also keep track of the highest accuracy for the lowest # nieghbors as our \"best\" model\n",
    "knn_highest_accuracy=0\n",
    "knn_lowest_neighbors=1\n",
    "\n",
    "# Loop through neighbor values, train and test each\n",
    "for n_neighbors in range(2, 25):   # Allow up to 50 neighbors increasing by 1 each loop\n",
    "    # Initialize and train the decision tree model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn_model.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # Collect info on training results if desired\n",
    "    # model_train_score = knn_model.score(X_train_scaled, y_train)                    # get the model accuracy\n",
    "    # print(\"{}\\t{}\".format(\"KNN Train (neighbors={})\".format(n_neighbors), \"{:.4f}\".format(model_test_score)))\n",
    "        \n",
    "    # Make predictions using training data\n",
    "    y_pred = knn_model.predict(X_test_pca)\n",
    "    model_test_score = accuracy_score(y_test, y_pred)\n",
    "    model_auc = roc_auc_score(y_test, y_pred)\n",
    "    model_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print the accuracies for each of the model params so far.   \n",
    "    print(\"KNN Test (neighbors={}, score={}, auc={}, f1={}\".format(n_neighbors, \"{:.4f}\".format(model_test_score),\n",
    "                                  \"{:.4f}\".format(model_auc), \"{:.4f}\".format(model_f1)))\n",
    "        \n",
    "    # we want the lowest number of estimators for better generalization \n",
    "    if model_test_score >= knn_highest_accuracy:\n",
    "        knn_lowest_neighbors = n_neighbors      # store lowest number of estimators\n",
    "        test_predictions = y_pred               # store test predictions\n",
    "        knn_best_model = knn_model              # store the best model\n",
    "        knn_highest_accuracy = model_test_score # update our highest score\n",
    "\n",
    "best_models.append(knn_best_model)              # add to our \"best model\" collection  \n",
    "model_accuracies.append(knn_highest_accuracy)   # add the accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: 14 neighbors , accuracy 0.8412\n",
      "MAE: 0.1587613035318205, AUC: 0.7355453214785367\n",
      "Precision: 0.7318982387475538, Recall: 0.5325738697045211\n",
      "Model Confusion Matrix:\n",
      " [[8365  548]\n",
      " [1313 1496]]\n",
      "Train data F1-Score for class '1': 0.6165258602926025\n",
      "Train data F1-Score for class '0': 0.8998978000107579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11722"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the best model info\n",
    "model_param_info =\"{} neighbors\".format(knn_lowest_neighbors)\n",
    "measure_model(model_param_info, knn_highest_accuracy, y_test, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Selection\n",
    "\n",
    "Here I enumerate through the best model for each type of classifier; do cross validation; and output AUC, accuracy, and info on the hyperparameters of our best of each type of model.  I'll use this to select the best model.\n",
    "\n",
    "The best model I will choose is the one with the highest AUC that has the lowest standard deviation and the one with the highest accuracy.\n",
    "\n",
    "After this I will load the test.csv we need to predict over, do predictions, encode the class label outputs of the predictions, and save the final csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.89 (+/- 0.01) [Accuracy 0.8386] LogisticRegression(random_state=17, solver='saga')\n",
      "ROC AUC: 0.88 (+/- 0.01) [Accuracy 0.8497] SVC(C=5.0, random_state=17)\n",
      "ROC AUC: 0.88 (+/- 0.01) [Accuracy 0.8369] DecisionTreeClassifier(criterion='log_loss', max_depth=7, random_state=17)\n",
      "ROC AUC: 0.90 (+/- 0.01) [Accuracy 0.8453] RandomForestClassifier(n_estimators=300, random_state=17)\n",
      "ROC AUC: 0.88 (+/- 0.01) [Accuracy 0.8412] KNeighborsClassifier(n_neighbors=14)\n"
     ]
    }
   ],
   "source": [
    "# Do some cross validation on all models using AUC\n",
    "for model, accuracy in zip(best_models, model_accuracies):\n",
    "        score = cross_val_score(estimator=model,\n",
    "                                X=X_train_pca,\n",
    "                                y=y_train,\n",
    "                                cv=10,\n",
    "                                scoring='roc_auc')\n",
    "        print(f'ROC AUC: {score.mean():.2f} '\n",
    "                f'(+/- {score.std():.2f}) [Accuracy {accuracy:.4f}]', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8567650571574816\n",
      "ROC AUC: 0.90 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# We'll choose our RF model but let's try it without PCA with 300 estimators and gini criterion\n",
    "random_forest_no_pca = RandomForestClassifier(criterion='gini', n_estimators=300, random_state=17)\n",
    "random_forest_no_pca.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions using training data\n",
    "y_pred = random_forest_no_pca.predict(X_test_std)\n",
    "model_test_score = accuracy_score(y_test, y_pred)\n",
    "model_auc = roc_auc_score(y_test, y_pred)\n",
    "model_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Score: {}\".format(model_test_score))\n",
    "score = cross_val_score(estimator=random_forest_no_pca,\n",
    "                        X=X_train_pca,\n",
    "                        y=y_train,\n",
    "                        cv=10,\n",
    "                        scoring='roc_auc')\n",
    "print(f'ROC AUC: {score.mean():.2f} ' f'(+/- {score.std():.2f})')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LDA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit a linear descriminant over the PCA. \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lda \u001b[38;5;241m=\u001b[39m \u001b[43mLDA\u001b[49m(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m X_train_pca_lda \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mfit_transform(X_train_std\u001b[38;5;241m.\u001b[39mtoarray(), y_train)\n\u001b[0;32m      4\u001b[0m X_test_pca_lda \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mtransform(X_test_std\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LDA' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit a linear descriminant over the PCA. \n",
    "lda = LDA(n_components=1)\n",
    "X_train_pca_lda = lda.fit_transform(X_train_std.toarray(), y_train)\n",
    "X_test_pca_lda = lda.transform(X_test_std.toarray())\n",
    "\n",
    "# I'll try the same thing but I'll do PCA with LDA applied after PCA dimensionality reduction.\n",
    "# We'll choose our RF model but let's try it without PCA with 300 estimators and gini criterion\n",
    "random_forest_pca_lda = RandomForestClassifier(criterion='gini', n_estimators=300, random_state=17)\n",
    "random_forest_pca_lda.fit(X_train_pca_lda, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_forest_pca_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions using training data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_forest_pca_lda\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test_pca_lda)\n\u001b[0;32m      3\u001b[0m y_pred\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mmodel_test_score = accuracy_score(y_test, y_pred)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mprint(\"Score: {}\".format(model_test_score))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mprint(f'ROC AUC: {score.mean():.2f} ' f'(+/- {score.std():.2f})')\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_forest_pca_lda' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions using training data\n",
    "y_pred = random_forest_pca_lda.predict(X_test_pca_lda)\n",
    "y_pred.size\n",
    "\n",
    "'''\n",
    "model_test_score = accuracy_score(y_test, y_pred)\n",
    "print(\"Score: {}\".format(model_test_score))\n",
    "score = cross_val_score(estimator=random_forest_pca_lda,\n",
    "                        X=X_train_pca,\n",
    "                        y=y_train,\n",
    "                        cv=10,\n",
    "                        scoring='roc_auc')\n",
    "print(f'ROC AUC: {score.mean():.2f} ' f'(+/- {score.std():.2f})')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model without PCA has no accuracy improvement but takes much longer to train and is much more complex.  I will select the dimensionally-reduced model (PCA applied version) of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict over our model and save the result\n",
    "\n",
    "My first submission to Kaggle was based on the best AUC of 0.99 +/- 0.01 with an expected accuracy of 0.9854 and used the Random Forest Classifier using the log_loss criterion with 300 estimators.  Below is from my second submission, which used the Logistic Regression model to get my final submission.\n",
    "\n",
    "I then did predictions using the choice model, which was saved as the lr_best_model. Then we do the inverse transform of the class labels on the output of the prediction, cobble together a resulting dataset for submission, and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  label\n",
      "0       392      0\n",
      "1      1900      0\n",
      "2     24507      0\n",
      "3     32817      0\n",
      "4     47893      0\n",
      "...     ...    ...\n",
      "9764  13000      0\n",
      "9765  43012      0\n",
      "9766  34782      0\n",
      "9767  23538      0\n",
      "9768  23097      0\n",
      "\n",
      "[9769 rows x 2 columns]\n",
      "csv written.\n"
     ]
    }
   ],
   "source": [
    "# Do predictions on the submission test set and save the output as csv\n",
    "data_test_input = pd.read_csv('datasets/test.csv') # get the test inputs\n",
    "\n",
    "# remove the ID column and the save it in output_ids\n",
    "output_ids = data_test_input['id']                  # set the IDs we'll output but don't predict on them\n",
    "X_test_cleansed = data_test_input.drop('id', axis=1)\n",
    "\n",
    "# Need to encode the test data\n",
    "# This uses the OHE and scaler we trained earlier as feature_encoder\n",
    "X_test_std = feature_encoder.transform(X_test_cleansed)\n",
    "\n",
    "# Use the previous pca matrix to transform the test data\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "# Now ready for predicting. Predict and store results.\n",
    "y_pred = rforest_best_model.predict(X_test_pca)   # use the model to predict outcomes\n",
    "\n",
    "output_df = pd.DataFrame(output_ids)                # prep a dataframe for our output\n",
    "output_df = output_df.assign(label=y_pred)  # append the predictions to the IDs\n",
    "\n",
    "output_df.to_csv(\"DonKrapohl_project2_submission.csv\", index=False) # write the csv\n",
    "\n",
    "print(output_df)\n",
    "\n",
    "print('csv written.')                               # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "My choice of models used Logistic Regression with C=0.75 employing the SAGA solver. Submitted to Kaggle it achieved 0.98245 accuracy.  My first submission used Random Forest and, while it scored well locally, it only got to 0.95614 over the final test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uwf-venv-breast-cancer",
   "language": "python",
   "name": "uwf-venv-breast-cancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
